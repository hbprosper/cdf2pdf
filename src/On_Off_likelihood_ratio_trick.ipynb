{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4762b2b3-822f-4d04-b266-05b556d074c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd\n",
    "import scipy as sp; import scipy.stats as st\n",
    "import torch; import torch.nn as nn\n",
    "#use numba's just-in-time compiler to speed things up\n",
    "from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler; from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp; import matplotlib.pyplot as plt; \n",
    "#reset matplotlib stle/parameters\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use('seaborn-deep')\n",
    "mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15; font_axes=15\n",
    "# %matplotlib inline\n",
    "from joblib import  Memory\n",
    "\n",
    "import copy; import sys; import os\n",
    "from IPython.display import Image, display\n",
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2563f8a4-5797-413c-8d95-a5fc2085dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTSIZE=18\n",
    "font = {'family': 'serif', 'weight':'normal', 'size':FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "mp.rc('text',usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94f8630-7ee3-4f03-8ad1-721b8252e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(func):\n",
    "    \"\"\"Print the function signature and return value\"\"\"\n",
    "    import functools\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_debug(*args, **kwargs):\n",
    "        args_repr = [repr(a) for a in args]\n",
    "        kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]\n",
    "        signature = \", \".join(args_repr + kwargs_repr)\n",
    "        print(f\"Calling {func.__name__}({signature})\")\n",
    "        values = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__!r} returned {values!r}\")\n",
    "        return values\n",
    "\n",
    "    return wrapper_debug\n",
    "\n",
    "def theta_hat_func(n,m, MLE):\n",
    "       #n,m are integer arrays\n",
    "    if MLE==True:\n",
    "        theta_hat = n-m\n",
    "    else:\n",
    "        # non-MLE\n",
    "        # theta_hat = n-m\n",
    "        # theta_hat = (theta_hat) * (theta_hat > 0)\n",
    "        theta_hat = np.where(n>m, n-m, 0)\n",
    "         \n",
    "    return theta_hat\n",
    "\n",
    "\n",
    "def L_prof_global(n,m, MLE):\n",
    "    #n,m integer arrays\n",
    "    # nu_hat = m, if theta_hat = theta_hat_MLE\n",
    "    # nu_hat  =  (m+n)/2 if theta_hat = n-m\n",
    "    # nu_hat = 0  if theta_hat != n-m\n",
    "    theta_hat=theta_hat_func(n,m,MLE)\n",
    "    # print('n-m ',  n-m)\n",
    "    if MLE==True:\n",
    "        # i.e. if theta_hat = n-m\n",
    "        # assert theta_hat==n-m\n",
    "        nu_hat = m\n",
    "    else:\n",
    "        nu_hat = np.where(theta_hat ==0, (m+n)/2, m)\n",
    "        # if theta_hat==0:\n",
    "        #     nu_hat =(m+n)/2\n",
    "        # else:\n",
    "        #     _hat = m\n",
    "        # # if theta_hat== n-m:\n",
    "        # #     nu_hat = (m+n)/2\n",
    "        # # else:\n",
    "        # #     nu_hat = 0\n",
    "        # # nu_hat = np.where(theta_hat==n-m,\n",
    "        # #                   (m+n)/2, \n",
    "        # #                   0)\n",
    "    p1=st.poisson.pmf(n, theta_hat+nu_hat)\n",
    "    p2 = st.poisson.pmf(m, nu_hat)\n",
    "    return p1*p2\n",
    "\n",
    "def L_theta_nu(n,m,theta,nu):\n",
    "    p1 = st.poisson.pmf(n, theta+nu)\n",
    "    p2 = st.poisson.pmf(m, nu)\n",
    "    return p1*p2\n",
    "def lambda_test_2d(n,m, theta, nu, MLE):\n",
    "    Ln= L_theta_nu(n,m,theta,nu)\n",
    "    \n",
    "    Ld= L_prof_global(n,m, MLE)\n",
    "    eps=1e-20\n",
    "    Ld=Ld+eps\n",
    "    lambda_  = -2*np.log(Ln/Ld)\n",
    "    return np.array(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "82571646-7105-417b-8050-e41bcfcc0a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaMin, thetaMax =  0, 20\n",
    "numin, numax = 0, 20\n",
    "Nmin, Nmax =  1,10\n",
    "Mmin, Mmax =  1 , 10\n",
    "\n",
    "def generate_training_data_LR(Bprime, MLE, save_data, sig_or_bkg):\n",
    "    \"\"\"Generate the training data, that is, features=[theta, nu, N, M], targets=Z\"\"\"\n",
    "    #sample theta and nu from uniform(0,20)\n",
    "    theta = st.uniform.rvs(thetaMin, thetaMax, size=Bprime)\n",
    "    # nu = st.uniform.rvs(nuMin, nuMax, size=Bprime)\n",
    "    nu= st.uniform.rvs(numin, numax, size=Bprime)\n",
    "    #n,m ~ F_{\\theta,\\nu}, ie our simulator. sample n from a Poisson with mean theta+nu \n",
    "    n = st.poisson.rvs(theta+ nu, size=Bprime)\n",
    "    #sample m from a poisson with mean nu\n",
    "    m = st.poisson.rvs(nu, size=Bprime)\n",
    "    #sample our observed counts (N,M), which take the place of D\n",
    "    \n",
    "    theta_O = st.uniform.rvs(thetaMin, thetaMax, size=Bprime)\n",
    "    # nu = st.uniform.rvs(nuMin, nuMax, size=Bprime)\n",
    "    nu_O= st.uniform.rvs(numin, numax, size=Bprime)\n",
    "    \n",
    "    N = st.poisson.rvs(theta_O+ nu_O, size=Bprime)\n",
    "    M = st.poisson.rvs(nu_O, size=Bprime)\n",
    "    \n",
    "    if sig_or_bkg=='sig':\n",
    "        \n",
    "        lambda_ = lambda_test_2d(n, m, theta, nu, MLE)\n",
    "        target = np.ones_like(lambda_)\n",
    "        df = pd.DataFrame({'theta':theta, 'nu': nu, 'lambda': lambda_, 'target': target})\n",
    "        \n",
    "    elif sig_or_bkg=='bkg':\n",
    "        \n",
    "        lambda_ = lambda_test_2d(N, M, theta_O, nu_O, MLE)\n",
    "        target = np.zeros_like(lambda_)\n",
    "        df = pd.DataFrame({'theta':theta, 'nu': nu, 'lambda': lambda_, 'target': target})\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "360954a0-4092-4d33-b5be-7a5423e4520a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "      <th>lambda</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991823</td>\n",
       "      <td>7.866949</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.553237</td>\n",
       "      <td>16.450391</td>\n",
       "      <td>0.503511</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.428888</td>\n",
       "      <td>2.485914</td>\n",
       "      <td>1.933862</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.518586</td>\n",
       "      <td>10.640593</td>\n",
       "      <td>0.738783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.848949</td>\n",
       "      <td>14.770042</td>\n",
       "      <td>0.148607</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       theta         nu    lambda  target\n",
       "0   0.991823   7.866949  0.004477     1.0\n",
       "1  17.553237  16.450391  0.503511     1.0\n",
       "2  18.428888   2.485914  1.933862     1.0\n",
       "3  15.518586  10.640593  0.738783     1.0\n",
       "4   3.848949  14.770042  0.148607     1.0"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sig = generate_training_data_LR(Bprime=100000, MLE=False, save_data=False, sig_or_bkg='sig')\n",
    "df_sig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d4e637b5-c439-46a7-964e-f8dbcd61b5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "      <th>lambda</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.922836</td>\n",
       "      <td>11.624329</td>\n",
       "      <td>0.871327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.514048</td>\n",
       "      <td>2.974509</td>\n",
       "      <td>0.850546</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.364677</td>\n",
       "      <td>7.712140</td>\n",
       "      <td>0.072733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.376020</td>\n",
       "      <td>3.958072</td>\n",
       "      <td>2.563019</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.521943</td>\n",
       "      <td>4.807600</td>\n",
       "      <td>5.587317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       theta         nu    lambda  target\n",
       "0  18.922836  11.624329  0.871327     0.0\n",
       "1   0.514048   2.974509  0.850546     0.0\n",
       "2   0.364677   7.712140  0.072733     0.0\n",
       "3   0.376020   3.958072  2.563019     0.0\n",
       "4   5.521943   4.807600  5.587317     0.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bkg = generate_training_data_LR(Bprime=100000, MLE=False, save_data=False, sig_or_bkg='bkg')\n",
    "df_bkg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2009c38-2e96-441e-abdd-7fc2283d775f",
   "metadata": {},
   "source": [
    "## Note that $\\lambda$ distribution looks smooth because it's over all $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "2fec98de-a29c-4ce9-80f6-b6faaaf0dcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df_sig['lambda'], label='signal',alpha=0.2, range=(0,15), bins=100)\n",
    "plt.hist(df_bkg['lambda'], label='background',alpha=0.2,range=(0,15),bins=100)\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.legend();plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "ffc48f98-833c-4f11-80bf-4fb4bbe6629c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "      <th>lambda</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.991823</td>\n",
       "      <td>7.866949</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17.553237</td>\n",
       "      <td>16.450391</td>\n",
       "      <td>0.503511</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.428888</td>\n",
       "      <td>2.485914</td>\n",
       "      <td>1.933862</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.518586</td>\n",
       "      <td>10.640593</td>\n",
       "      <td>0.738783</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.848949</td>\n",
       "      <td>14.770042</td>\n",
       "      <td>0.148607</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.687766</td>\n",
       "      <td>14.440746</td>\n",
       "      <td>0.246170</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>18.215306</td>\n",
       "      <td>7.369372</td>\n",
       "      <td>5.095340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>17.500933</td>\n",
       "      <td>0.436567</td>\n",
       "      <td>2.136865</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>16.686499</td>\n",
       "      <td>17.470170</td>\n",
       "      <td>4.378726</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>9.905309</td>\n",
       "      <td>15.357707</td>\n",
       "      <td>1.287198</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           theta         nu    lambda  target\n",
       "0       0.991823   7.866949  0.004477     1.0\n",
       "1      17.553237  16.450391  0.503511     1.0\n",
       "2      18.428888   2.485914  1.933862     1.0\n",
       "3      15.518586  10.640593  0.738783     1.0\n",
       "4       3.848949  14.770042  0.148607     1.0\n",
       "...          ...        ...       ...     ...\n",
       "99995   1.687766  14.440746  0.246170     0.0\n",
       "99996  18.215306   7.369372  5.095340     0.0\n",
       "99997  17.500933   0.436567  2.136865     0.0\n",
       "99998  16.686499  17.470170  4.378726     0.0\n",
       "99999   9.905309  15.357707  1.287198     0.0\n",
       "\n",
       "[200000 rows x 4 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat([df_sig, df_bkg])\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "5c5ea7de-753d-4c42-9c81-c60daa7c364a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "      <th>lambda</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70869</th>\n",
       "      <td>17.000057</td>\n",
       "      <td>7.665552</td>\n",
       "      <td>0.446099</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37311</th>\n",
       "      <td>2.535791</td>\n",
       "      <td>3.808729</td>\n",
       "      <td>1.982458</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55735</th>\n",
       "      <td>13.223618</td>\n",
       "      <td>10.353130</td>\n",
       "      <td>1.836652</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98432</th>\n",
       "      <td>0.934153</td>\n",
       "      <td>2.393948</td>\n",
       "      <td>1.359203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87151</th>\n",
       "      <td>0.334119</td>\n",
       "      <td>3.089969</td>\n",
       "      <td>0.747550</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>10.209048</td>\n",
       "      <td>11.825211</td>\n",
       "      <td>2.309944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42916</th>\n",
       "      <td>4.696587</td>\n",
       "      <td>6.065527</td>\n",
       "      <td>3.576147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72514</th>\n",
       "      <td>19.954664</td>\n",
       "      <td>8.529999</td>\n",
       "      <td>0.599237</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77543</th>\n",
       "      <td>9.529418</td>\n",
       "      <td>19.387557</td>\n",
       "      <td>6.879117</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46421</th>\n",
       "      <td>13.388594</td>\n",
       "      <td>5.755863</td>\n",
       "      <td>3.284531</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           theta         nu    lambda  target\n",
       "70869  17.000057   7.665552  0.446099     1.0\n",
       "37311   2.535791   3.808729  1.982458     0.0\n",
       "55735  13.223618  10.353130  1.836652     1.0\n",
       "98432   0.934153   2.393948  1.359203     0.0\n",
       "87151   0.334119   3.089969  0.747550     1.0\n",
       "...          ...        ...       ...     ...\n",
       "1994   10.209048  11.825211  2.309944     1.0\n",
       "42916   4.696587   6.065527  3.576147     0.0\n",
       "72514  19.954664   8.529999  0.599237     0.0\n",
       "77543   9.529418  19.387557  6.879117     1.0\n",
       "46421  13.388594   5.755863  3.284531     1.0\n",
       "\n",
       "[200000 rows x 4 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ec271798-2233-433c-a2da-d01cd42e0698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_t_x(df, target, source):\n",
    "    # change from pandas dataframe format to a numpy \n",
    "    # array of the specified types\n",
    "    t = np.array(df[target])\n",
    "    x = np.array(df[source])\n",
    "    return t, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "733432a0-011e-425f-985b-512bc896dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_training_batch(x, t, batch_size):\n",
    "    # the numpy function choice(length, number)\n",
    "    # selects at random \"batch_size\" integers from \n",
    "    # the range [0, length-1] corresponding to the\n",
    "    # row indices.\n",
    "    rows    = np.random.choice(len(x), batch_size)\n",
    "    batch_x = x[rows]\n",
    "    batch_t = t[rows]\n",
    "    # batch_x.T[-1] = np.random.uniform(0, 1, batch_size)\n",
    "    return (batch_x, batch_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b63bfbc6-56e4-47af-934c-8d7fc4c18dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getwholedata_LRT():\n",
    "    \"\"\" Get train test split arrays\"\"\"\n",
    "    \n",
    "    data = df_train\n",
    "        \n",
    "    train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "    #split the train data (0.8 of whole set) again into 0.8*0.8=0.64 of whole set\n",
    "    \n",
    "\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data  = test_data.reset_index(drop=True)\n",
    "\n",
    "    target='target'\n",
    "    # source = ['theta','nu','theta_hat','N','M']\n",
    "\n",
    "    source = ['theta', 'nu', 'lambda']\n",
    "\n",
    "    train_t, train_x = split_t_x(train_data, target=target, source=source)\n",
    "    test_t,  test_x  = split_t_x(test_data,  target=target, source=source)\n",
    "    print('train_t shape = ', train_t.shape, '\\n')\n",
    "    print('train_x shape = ', train_x.shape, '\\n')\n",
    "    \n",
    "    # if valid:\n",
    "        #if you want to also make a validation data set\n",
    "    train_data, valid_data = train_test_split(train_data, test_size=0.2)\n",
    "    valid_data = valid_data.reset_index(drop=True)\n",
    "    valid_t, valid_x = split_t_x(valid_data, target=target, source=source)\n",
    "\n",
    "        \n",
    "    return train_t, train_x, test_t,  test_x, valid_t, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "87f918d8-8981-4ec7-a99c-76cca721f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_t shape =  (160000,) \n",
      "\n",
      "train_x shape =  (160000, 3) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_t, train_x, test_t,  test_x, valid_t, valid_x = getwholedata_LRT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0313c70c-ae95-45f1-98ba-53f220706711",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLURegressionModel(nn.Module):\n",
    "    #inherit from the super class\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                #Dropout seems to worsen model performance\n",
    "                # layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.SiLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                #Dropout seems to worsen model performance\n",
    "                # layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.SiLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "\n",
    "        # ONLY IF ITS A CLASSIFICATION, ADD SIGMOID\n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f7c0650c-be43-4565-b947-1ccf751ee67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@debug\n",
    "def load_untrained_model(PARAMS):\n",
    "    \"\"\"Load an untrained model (with weights initiatted) according to model paramateters in the \n",
    "    PARAMS dictionary\n",
    "\n",
    "    Args:\n",
    "        PARAMS (dict): dictionary of model/training parameters: i.e. hyperparameters and training parameters.\n",
    "\n",
    "    Returns:\n",
    "        utils.RegularizedRegressionModel object\n",
    "    \"\"\"\n",
    "    model = SiLURegressionModel(\n",
    "        nfeatures=PARAMS['NFEATURES'],\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout=PARAMS[\"dropout\"],\n",
    "        # activation=PARAMS[\"activation\"]\n",
    "    )\n",
    "    # model.apply(initialize_weights)\n",
    "    print('INITIATED UNTRAINED MODEL:',\n",
    "          # model\n",
    "         )\n",
    "    # print(model)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "99f0238e-f21e-4ea2-8b27-10e8bbafab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, PARAMS, pth_string):\n",
    "    models_path = os.path.join(os.getcwd(), '../models')\n",
    "    PATH=os.path.join(models_path, pth_string)\n",
    "    model = SiLURegressionModel(\n",
    "        nfeatures=PARAMS['NFEATURES'],\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout=PARAMS[\"dropout\"]\n",
    "    )\n",
    "    checkpoint = torch.load(PATH)\n",
    "    print('INITIATED MODEL:',  model)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f'loading model with th string : {pth_string}\\n')    \n",
    "    print(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c6107f94-7f16-4608-af89-438e3f9bbfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_quadratic_loss(f, t, x):\n",
    "    # f and t must be of the same shape\n",
    "\n",
    "    # inv = torch.where(t !=0, 1/torch.abs(t), 1)\n",
    "    return  torch.mean((f - t)**2)\n",
    "\n",
    "def validate(model, avloss, inputs, targets):\n",
    "    # make sure we set evaluation mode so that any training specific\n",
    "    # operations are disabled.\n",
    "    model.eval() # evaluation mode\n",
    "    \n",
    "    with torch.no_grad(): # no need to compute gradients wrt. x and t\n",
    "        x = torch.from_numpy(inputs).float()\n",
    "        t = torch.from_numpy(targets).float()\n",
    "        # remember to reshape!\n",
    "        o = model(x).reshape(t.shape)\n",
    "    return avloss(o, t, x)\n",
    "def train(model, optimizer, avloss,\n",
    "          batch_size, \n",
    "          n_iterations, traces, \n",
    "          step, window, MLE, with_lambda_D):\n",
    "    \n",
    "    # to keep track of average losses\n",
    "    xx, yy_t, yy_v, yy_v_avg = traces\n",
    "    \n",
    "\n",
    "    \n",
    "    if MLE==True:\n",
    "        train_t, train_x, test_t,  test_x, _, _ = getwholedata_LRT()\n",
    "    else:\n",
    "        train_t, train_x, test_t,  test_x, _, _ = getwholedata_LRT()\n",
    "        \n",
    "    n = len(test_x)\n",
    "    print('Iteration vs average loss')\n",
    "    print(\"%10s\\t%10s\\t%10s\" % \\\n",
    "          ('iteration', 'train-set', 'valid-set'))\n",
    "    \n",
    "    # training_set_features, training_set_targets, evaluation_set_features, evaluation_set_targets = get_data_sets(simulate_data=False, batchsize=batch_size)\n",
    "    \n",
    "    for ii in range(n_iterations):\n",
    "\n",
    "        # set mode to training so that training specific \n",
    "        # operations such as dropout are enabled.\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        # get a random sample (a batch) of data (as numpy arrays)\n",
    "        \n",
    "        #Harrison-like Loader\n",
    "        batch_x, batch_t = get_features_training_batch(train_x, train_t, batch_size)\n",
    "        \n",
    "        #Or Ali's Loader\n",
    "        # batch_x, batch_t = next(training_set_features()), next(training_set_targets())\n",
    "        # batch_x_eval, batch_t_eval = next(evaluation_set_features()), next(evaluation_set_targets())\n",
    "\n",
    "        with torch.no_grad(): # no need to compute gradients \n",
    "            # wrt. x and t\n",
    "            x = torch.from_numpy(batch_x).float()\n",
    "            t = torch.from_numpy(batch_t).float()      \n",
    "\n",
    "\n",
    "        outputs = model(x).reshape(t.shape)\n",
    "   \n",
    "        # compute a noisy approximation to the average loss\n",
    "        empirical_risk = avloss(outputs, t, x)\n",
    "        \n",
    "        # use automatic differentiation to compute a \n",
    "        # noisy approximation of the local gradient\n",
    "        optimizer.zero_grad()       # clear previous gradients\n",
    "        empirical_risk.backward()   # compute gradients\n",
    "        \n",
    "        # finally, advance one step in the direction of steepest \n",
    "        # descent, using the noisy local gradient. \n",
    "        optimizer.step()            # move one step\n",
    "        \n",
    "        if ii % step == 0:\n",
    "            \n",
    "            \n",
    "            #using Harrison-like loader\n",
    "            acc_t = validate(model, avloss, train_x[:n], train_t[:n]) \n",
    "            acc_v = validate(model, avloss, test_x[:n], test_t[:n])\n",
    "            \n",
    "            #using Ali's loader\n",
    "            # acc_t = validate(model, avloss, batch_x, batch_t) \n",
    "            # acc_v = validate(model, avloss, batch_x_eval, batch_t_eval)\n",
    "            \n",
    "\n",
    "            yy_t.append(acc_t)\n",
    "            yy_v.append(acc_v)\n",
    "            \n",
    "            # compute running average for validation data\n",
    "            len_yy_v = len(yy_v)\n",
    "            if   len_yy_v < window:\n",
    "                yy_v_avg.append( yy_v[-1] )\n",
    "            elif len_yy_v == window:\n",
    "                yy_v_avg.append( sum(yy_v) / window )\n",
    "            else:\n",
    "                acc_v_avg  = yy_v_avg[-1] * window\n",
    "                acc_v_avg += yy_v[-1] - yy_v[-window-1]\n",
    "                yy_v_avg.append(acc_v_avg / window)\n",
    "                        \n",
    "            if len(xx) < 1:\n",
    "                xx.append(0)\n",
    "                print(\"%10d\\t%10.6f\\t%10.6f\" % \\\n",
    "                      (xx[-1], yy_t[-1], yy_v[-1]))\n",
    "            else:\n",
    "                xx.append(xx[-1] + step)\n",
    "                    \n",
    "                print(\"\\r%10d\\t%10.6f\\t%10.6f\\t%10.6f\" % \\\n",
    "                          (xx[-1], yy_t[-1], yy_v[-1], yy_v_avg[-1]), \n",
    "                      end='')\n",
    "            \n",
    "    print()      \n",
    "    return (xx, yy_t, yy_v, yy_v_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e2b426dd-57e1-4544-a945-8062cb8ccfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_LRT_nonMLE_SILU = {\n",
    "\"n_layers\": int(12),\n",
    "\"hidden_size\": int(6),\n",
    "\"dropout\": float(0.13),\n",
    "\"NFEATURES\":int(3),\n",
    "\"activation\": \"SiLU\",\n",
    "'optimizer_name':'NAdam',\n",
    "    # 'optimizer_name':'RMSprop',\n",
    "'starting_learning_rate':float(0.0006),\n",
    "'momentum':float(0.9),\n",
    "'batch_size':int(256),\n",
    "'n_iterations': int(1e4),\n",
    "'traces_step':int(100),\n",
    "'L2':float(0.1),\n",
    "'MLE':False,\n",
    "'with_lambda_D':True,\n",
    "'pth_string':'FEB_20_model_lambda_D_nonMLE_SILU.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4ec1ab74-51a9-49d3-a756-824309151cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling load_untrained_model({'n_layers': 12, 'hidden_size': 6, 'dropout': 0.13, 'NFEATURES': 3, 'activation': 'SiLU', 'optimizer_name': 'NAdam', 'starting_learning_rate': 0.0006, 'momentum': 0.9, 'batch_size': 256, 'n_iterations': 10000, 'traces_step': 100, 'L2': 0.1, 'MLE': False, 'with_lambda_D': True, 'pth_string': 'FEB_20_model_lambda_D_nonMLE_SILU.pth'})\n",
      "INITIATED UNTRAINED MODEL:\n",
      "'load_untrained_model' returned SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=6, bias=True)\n",
      "    (1): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (4): SiLU()\n",
      "    (5): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (6): SiLU()\n",
      "    (7): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (10): SiLU()\n",
      "    (11): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (12): SiLU()\n",
      "    (13): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (16): SiLU()\n",
      "    (17): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (18): SiLU()\n",
      "    (19): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (22): SiLU()\n",
      "    (23): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (24): SiLU()\n",
      "    (25): Linear(in_features=6, out_features=1, bias=True)\n",
      "    (26): Sigmoid()\n",
      "  )\n",
      ")\n",
      "train_t shape =  (160000,) \n",
      "\n",
      "train_x shape =  (160000, 3) \n",
      "\n",
      "Iteration vs average loss\n",
      " iteration\t train-set\t valid-set\n",
      "         0\t  0.251147\t  0.251215\n",
      "      8000\t  0.249982\t  0.249956\t  0.249956\n"
     ]
    }
   ],
   "source": [
    "untrained_LRT_model_nonMLE = load_untrained_model(PARAMS_LRT_nonMLE_SILU)\n",
    "\n",
    "BATCHSIZE=PARAMS_LRT_nonMLE_SILU[\"batch_size\"]\n",
    "traces_SiLU = ([], [], [], [])\n",
    "traces_step = 2000\n",
    "optimizer_name=PARAMS_LRT_nonMLE_SILU[\"optimizer_name\"]\n",
    "\n",
    "optimizer_SiLU = getattr(torch.optim, str(optimizer_name))(untrained_LRT_model_nonMLE.parameters(), \n",
    "                                                           lr=PARAMS_LRT_nonMLE_SILU[\"starting_learning_rate\"])\n",
    "\n",
    "traces_SiLU = train(model=untrained_LRT_model_nonMLE, \n",
    "              optimizer=optimizer_SiLU, \n",
    "              avloss=average_quadratic_loss,\n",
    "              batch_size=BATCHSIZE, \n",
    "              n_iterations=PARAMS_LRT_nonMLE_SILU[\"n_iterations\"], \n",
    "              traces=traces_SiLU, \n",
    "              step=traces_step, \n",
    "              window=200,\n",
    "                MLE=False,\n",
    "                with_lambda_D=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "02c20b3a-90f1-49e5-9194-50dbf2ea4ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.562323581720001"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "410dbd54-3e00-4dce-9680-0ba7b767317b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X(theta, nu, N_points, MLE):\n",
    "    \n",
    "    N = st.poisson.rvs(theta+nu, size=N_points)\n",
    "    M = st.poisson.rvs(nu, size=N_points)\n",
    "    lambda_D = lambda_test_2d(N, M, theta, nu, MLE)#.flatten()\n",
    "    X_sig = np.empty((N_points, 3))\n",
    "    X_sig[:,0] = np.ones_like(theta)*theta\n",
    "    X_sig[:,1] = np.ones_like(nu)*nu\n",
    "    X_sig[:,2]  = lambda_D\n",
    "    sig_X = torch.Tensor(X_sig)\n",
    "    \n",
    "    theta_0 =np.random.uniform(0,20)\n",
    "    nu_0 = np.random.uniform(0,20)\n",
    "    N_0 = st.poisson.rvs(theta_0+nu_0, size=N_points)\n",
    "    M_0 = st.poisson.rvs(nu_0, size=N_points)\n",
    "    g=[]\n",
    "    lambda_0 = lambda_test_2d(N_0, M_0, theta_0, nu_0, MLE)#.flatten()\n",
    "    g.append(lambda_0)\n",
    "    N_1 = st.poisson.rvs(theta_0+nu_0, size=N_points)\n",
    "    M_1 = st.poisson.rvs(nu_0, size=N_points)\n",
    "    lambda_1 = lambda_test_2d(N_1, M_1, theta_0, nu_0, MLE)\n",
    "    g.append(lambda_1)\n",
    "    \n",
    "    \n",
    "    X_bkg = np.empty((N_points, 3))\n",
    "    X_bkg[:,0] = np.ones_like(theta_0)*theta_0\n",
    "    X_bkg[:,1] = np.ones_like(nu_0)*nu_0\n",
    "    X_bkg[:,2]  = lambda_0\n",
    "    bkg_X = torch.Tensor(X_bkg)\n",
    "    \n",
    "    eval_array = np.vstack((sig_X, bkg_X))\n",
    "    \n",
    "    \n",
    "    return np.array(g).flatten(), torch.Tensor(eval_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "416958fa-0b5e-4b4a-87fd-e74366badb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaMin, thetaMax =  0, 20\n",
    "numin, numax = 0, 20\n",
    "Nmin, Nmax =  1,10\n",
    "Mmin, Mmax =  1 , 10\n",
    "\n",
    "def make_eval_tensor(Bprime, MLE):\n",
    "    #sample theta and nu from uniform(0,20)\n",
    "    g_l = []\n",
    "    sig_df = generate_training_data_LR(Bprime=Bprime, MLE=False, save_data=False, sig_or_bkg='sig')\n",
    "    bkg_df = generate_training_data_LR(Bprime=Bprime, MLE=False, save_data=False, sig_or_bkg='bkg')\n",
    "    g_l.append(bkg_df['lambda'])\n",
    "    bkg_df_2 = generate_training_data_LR(Bprime=Bprime, MLE=False, save_data=False, sig_or_bkg='bkg')\n",
    "    g_l.append(bkg_df_2['lambda'])\n",
    "    g = np.array(g_l).flatten()\n",
    "    \n",
    "    df = pd.concat([sig_df, bkg_df])\n",
    "    df.sample(frac=1)\n",
    "    df_np = df[['theta','nu', 'lambda']].to_numpy()\n",
    "    return df, g, torch.Tensor(df_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "7738e3d8-e417-4ebb-b3e0-988a6391d3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, g, eval_tensor = make_eval_tensor(Bprime=1000, MLE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "990567e1-3641-49ac-b45c-95e9cdffb2b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2000, 3])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g, eval_tensor=generate_X(theta=19, nu=5, N_points=1000, MLE=False)\n",
    "eval_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "af1576da-5d2a-46da-9a2b-60df30ee38df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0007964e-fd82-49cc-ab98-1d05cdca34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = untrained_LRT_model_nonMLE(eval_tensor).view(-1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f92944-71df-4c86-8b50-b5361b365908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8209faca-c398-493d-b729-4ea2a83dbd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "87f226c4-cbbf-47fe-af93-4a2aecdc1859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "da5d8f92-7bc4-4026-a50a-10b3e311f544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = g * D/(1-D)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d47ddd33-7730-4a7b-86ce-52aee85ab819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['lambda'], alpha=0.3, bins=30, range=(0,30),density=True);\n",
    "plt.hist(f,alpha=0.3, bins=30, range=(0,30),density=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc0c94f-06ba-4ae0-9d26-b3c47b159242",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
