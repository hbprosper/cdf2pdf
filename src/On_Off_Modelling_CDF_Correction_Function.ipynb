{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ce2611-4ac7-40c1-866b-c55ecb0f20dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; import pandas as pd\n",
    "import scipy as sp; import scipy.stats as st\n",
    "import torch; import torch.nn as nn\n",
    "#use numba's just-in-time compiler to speed things up\n",
    "from numba import njit\n",
    "from sklearn.preprocessing import StandardScaler; from sklearn.model_selection import train_test_split\n",
    "import matplotlib as mp; import matplotlib.pyplot as plt; \n",
    "#reset matplotlib stle/parameters\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.style.use('seaborn-deep')\n",
    "mp.rcParams['agg.path.chunksize'] = 10000\n",
    "font_legend = 15; font_axes=15\n",
    "# %matplotlib inline\n",
    "from joblib import  Memory\n",
    "\n",
    "import copy; import sys; import os\n",
    "from IPython.display import Image, display\n",
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce450f56-d118-448b-a3a2-48e8e54fecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTSIZE=18\n",
    "font = {'family': 'serif', 'weight':'normal', 'size':FONTSIZE}\n",
    "mp.rc('font', **font)\n",
    "mp.rc('text',usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "079adc6f-957c-4b90-8cc1-60bd5140ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(func):\n",
    "    \"\"\"Print the function signature and return value\"\"\"\n",
    "    import functools\n",
    "\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_debug(*args, **kwargs):\n",
    "        args_repr = [repr(a) for a in args]\n",
    "        kwargs_repr = [f\"{k}={v!r}\" for k, v in kwargs.items()]\n",
    "        signature = \", \".join(args_repr + kwargs_repr)\n",
    "        print(f\"Calling {func.__name__}({signature})\")\n",
    "        values = func(*args, **kwargs)\n",
    "        print(f\"{func.__name__!r} returned {values!r}\")\n",
    "        return values\n",
    "\n",
    "    return wrapper_debug\n",
    "\n",
    "def theta_hat_func(n,m, MLE):\n",
    "       #n,m are integer arrays\n",
    "    if MLE==True:\n",
    "        theta_hat = n-m\n",
    "    else:\n",
    "        # non-MLE\n",
    "        # theta_hat = n-m\n",
    "        # theta_hat = (theta_hat) * (theta_hat > 0)\n",
    "        theta_hat = np.where(n>m, n-m, 0)\n",
    "         \n",
    "    return theta_hat\n",
    "\n",
    "\n",
    "def L_prof_global(n,m, MLE):\n",
    "    #n,m integer arrays\n",
    "    # nu_hat = m, if theta_hat = theta_hat_MLE\n",
    "    # nu_hat  =  (m+n)/2 if theta_hat = n-m\n",
    "    # nu_hat = 0  if theta_hat != n-m\n",
    "    theta_hat=theta_hat_func(n,m,MLE)\n",
    "    # print('n-m ',  n-m)\n",
    "    if MLE==True:\n",
    "        # i.e. if theta_hat = n-m\n",
    "        # assert theta_hat==n-m\n",
    "        nu_hat = m\n",
    "    else:\n",
    "        nu_hat = np.where(theta_hat ==0, (m+n)/2, m)\n",
    "        # if theta_hat==0:\n",
    "        #     nu_hat =(m+n)/2\n",
    "        # else:\n",
    "        #     _hat = m\n",
    "        # # if theta_hat== n-m:\n",
    "        # #     nu_hat = (m+n)/2\n",
    "        # # else:\n",
    "        # #     nu_hat = 0\n",
    "        # # nu_hat = np.where(theta_hat==n-m,\n",
    "        # #                   (m+n)/2, \n",
    "        # #                   0)\n",
    "    p1=st.poisson.pmf(n, theta_hat+nu_hat)\n",
    "    p2 = st.poisson.pmf(m, nu_hat)\n",
    "    return p1*p2\n",
    "\n",
    "def L_theta_nu(n,m,theta,nu):\n",
    "    p1 = st.poisson.pmf(n, theta+nu)\n",
    "    p2 = st.poisson.pmf(m, nu)\n",
    "    return p1*p2\n",
    "def lambda_test_2d(n,m, theta, nu, MLE):\n",
    "    Ln= L_theta_nu(n,m,theta,nu)\n",
    "    \n",
    "    Ld= L_prof_global(n,m, MLE)\n",
    "    eps=1e-20\n",
    "    Ld=Ld+eps\n",
    "    lambda_  = -2*np.log(Ln/Ld)\n",
    "    return np.array(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762e97ac-a15b-4e63-84a9-0916059dad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLURegressionModel(nn.Module):\n",
    "    #inherit from the super class\n",
    "    def __init__(self, nfeatures, ntargets, nlayers, hidden_size, dropout):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers):\n",
    "            if len(layers) ==0:\n",
    "                #inital layer has to have size of input features as its input layer\n",
    "                #its output layer can have any size but it must match the size of the input layer of the next linear layer\n",
    "                #here we choose its output layer as the hidden size (fully connected)\n",
    "                layers.append(nn.Linear(nfeatures, hidden_size))\n",
    "                #batch normalization\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                #Dropout seems to worsen model performance\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                #ReLU activation \n",
    "                layers.append(nn.SiLU())\n",
    "            else:\n",
    "                #if this is not the first layer (we dont have layers)\n",
    "                layers.append(nn.Linear(hidden_size, hidden_size))\n",
    "                # layers.append(nn.BatchNorm1d(hidden_size))\n",
    "                #Dropout seems to worsen model performance\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "                layers.append(nn.SiLU())\n",
    "                #output layer:\n",
    "        layers.append(nn.Linear(hidden_size, ntargets)) \n",
    "\n",
    "        # ONLY IF ITS A CLASSIFICATION, ADD SIGMOID\n",
    "        layers.append(nn.Sigmoid())\n",
    "            #we have defined sequential model using the layers in oulist \n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e74602-81bd-4bf0-a51a-344efc951c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@debug\n",
    "def load_untrained_model(PARAMS):\n",
    "    \"\"\"Load an untrained model (with weights initiatted) according to model paramateters in the \n",
    "    PARAMS dictionary\n",
    "\n",
    "    Args:\n",
    "        PARAMS (dict): dictionary of model/training parameters: i.e. hyperparameters and training parameters.\n",
    "\n",
    "    Returns:\n",
    "        utils.RegularizedRegressionModel object\n",
    "    \"\"\"\n",
    "    model = SiLURegressionModel(\n",
    "        nfeatures=PARAMS['NFEATURES'],\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout=PARAMS[\"dropout\"],\n",
    "        # activation=PARAMS[\"activation\"]\n",
    "    )\n",
    "    # model.apply(initialize_weights)\n",
    "    print('INITIATED UNTRAINED MODEL:',\n",
    "          # model\n",
    "         )\n",
    "    # print(model)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31b79c70-c485-48cd-bf6c-14ec2912e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_lambdaD_nonMLE_SILU = {\n",
    "\"n_layers\": int(12),\n",
    "\"hidden_size\": int(12),\n",
    "\"dropout\": float(0.13),\n",
    "\"NFEATURES\":int(3),\n",
    "\"activation\": \"SiLU\",\n",
    "'optimizer_name':'NAdam',\n",
    "    # 'optimizer_name':'RMSprop',\n",
    "'starting_learning_rate':float(0.0006),\n",
    "'momentum':float(0.9),\n",
    "'batch_size':int(256*2),\n",
    "'n_iterations': int(1e5),\n",
    "'traces_step':int(100),\n",
    "'L2':float(0.1),\n",
    "'MLE':False,\n",
    "'with_lambda_D':True,\n",
    "'pth_string':'FEB_20_model_lambda_D_nonMLE_SILU.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c8b25d-b1e4-4e73-a0dc-1ec170831683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling load_untrained_model({'n_layers': 12, 'hidden_size': 12, 'dropout': 0.13, 'NFEATURES': 3, 'activation': 'SiLU', 'optimizer_name': 'NAdam', 'starting_learning_rate': 0.0006, 'momentum': 0.9, 'batch_size': 512, 'n_iterations': 100000, 'traces_step': 100, 'L2': 0.1, 'MLE': False, 'with_lambda_D': True, 'pth_string': 'FEB_20_model_lambda_D_nonMLE_SILU.pth'})\n",
      "INITIATED UNTRAINED MODEL:\n",
      "'load_untrained_model' returned SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0.13, inplace=False)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (4): Dropout(p=0.13, inplace=False)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (7): Dropout(p=0.13, inplace=False)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (10): Dropout(p=0.13, inplace=False)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (13): Dropout(p=0.13, inplace=False)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (16): Dropout(p=0.13, inplace=False)\n",
      "    (17): SiLU()\n",
      "    (18): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (19): Dropout(p=0.13, inplace=False)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (22): Dropout(p=0.13, inplace=False)\n",
      "    (23): SiLU()\n",
      "    (24): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (25): Dropout(p=0.13, inplace=False)\n",
      "    (26): SiLU()\n",
      "    (27): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (28): Dropout(p=0.13, inplace=False)\n",
      "    (29): SiLU()\n",
      "    (30): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (31): Dropout(p=0.13, inplace=False)\n",
      "    (32): SiLU()\n",
      "    (33): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (34): Dropout(p=0.13, inplace=False)\n",
      "    (35): SiLU()\n",
      "    (36): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (37): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "untrained_SiLU_model_nonMLE = load_untrained_model(PARAMS_lambdaD_nonMLE_SILU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6284ef9c-3fb6-43a3-8d1a-cd8d51c1f475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, PARAMS, pth_string):\n",
    "    models_path = os.path.join(os.getcwd(), '../models')\n",
    "    PATH=os.path.join(models_path, pth_string)\n",
    "    model = SiLURegressionModel(\n",
    "        nfeatures=PARAMS['NFEATURES'],\n",
    "        ntargets=1,\n",
    "        nlayers=PARAMS[\"n_layers\"],\n",
    "        hidden_size=PARAMS[\"hidden_size\"],\n",
    "        dropout=PARAMS[\"dropout\"]\n",
    "    )\n",
    "    checkpoint = torch.load(PATH)\n",
    "    print('INITIATED MODEL:',  model)\n",
    "    \n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f'loading model with th string : {pth_string}\\n')    \n",
    "    print(model)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c18ca8a4-a00a-4e08-8933-6c6551ffe099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIATED MODEL: SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0.13, inplace=False)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (4): Dropout(p=0.13, inplace=False)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (7): Dropout(p=0.13, inplace=False)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (10): Dropout(p=0.13, inplace=False)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (13): Dropout(p=0.13, inplace=False)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (16): Dropout(p=0.13, inplace=False)\n",
      "    (17): SiLU()\n",
      "    (18): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (19): Dropout(p=0.13, inplace=False)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (22): Dropout(p=0.13, inplace=False)\n",
      "    (23): SiLU()\n",
      "    (24): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (25): Dropout(p=0.13, inplace=False)\n",
      "    (26): SiLU()\n",
      "    (27): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (28): Dropout(p=0.13, inplace=False)\n",
      "    (29): SiLU()\n",
      "    (30): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (31): Dropout(p=0.13, inplace=False)\n",
      "    (32): SiLU()\n",
      "    (33): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (34): Dropout(p=0.13, inplace=False)\n",
      "    (35): SiLU()\n",
      "    (36): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (37): Sigmoid()\n",
      "  )\n",
      ")\n",
      "loading model with th string : FEB_20_model_lambda_D_nonMLE_SILU.pth\n",
      "\n",
      "SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0.13, inplace=False)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (4): Dropout(p=0.13, inplace=False)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (7): Dropout(p=0.13, inplace=False)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (10): Dropout(p=0.13, inplace=False)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (13): Dropout(p=0.13, inplace=False)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (16): Dropout(p=0.13, inplace=False)\n",
      "    (17): SiLU()\n",
      "    (18): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (19): Dropout(p=0.13, inplace=False)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (22): Dropout(p=0.13, inplace=False)\n",
      "    (23): SiLU()\n",
      "    (24): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (25): Dropout(p=0.13, inplace=False)\n",
      "    (26): SiLU()\n",
      "    (27): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (28): Dropout(p=0.13, inplace=False)\n",
      "    (29): SiLU()\n",
      "    (30): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (31): Dropout(p=0.13, inplace=False)\n",
      "    (32): SiLU()\n",
      "    (33): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (34): Dropout(p=0.13, inplace=False)\n",
      "    (35): SiLU()\n",
      "    (36): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (37): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# untrained_SiLU_model = load_untrained_model(PARAMS_lambdaD_nonMLE_SILU)\n",
    "\n",
    "trained_SiLU_model_nonMLE = load_model(model = untrained_SiLU_model_nonMLE, \n",
    "                                PARAMS=PARAMS_lambdaD_nonMLE_SILU,\n",
    "                   pth_string=PARAMS_lambdaD_nonMLE_SILU[\"pth_string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9fe14b5-42bb-45d6-8267-297af9b36bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS_lambdaD_MLE_SILU = {\n",
    "\"n_layers\": int(12),\n",
    "\"hidden_size\": int(12),\n",
    "\"dropout\": float(0.13),\n",
    "\"NFEATURES\":int(3),\n",
    "\"activation\": \"SiLU\",\n",
    "'optimizer_name':'NAdam',\n",
    "    # 'optimizer_name':'RMSprop',\n",
    "'starting_learning_rate':float(0.0006),\n",
    "'momentum':float(0.9),\n",
    "'batch_size':int(256*2),\n",
    "'n_iterations': int(6e4),\n",
    "'traces_step':int(100),\n",
    "'L2':float(0.1),\n",
    "'MLE':True,\n",
    "'with_lambda_D':True,\n",
    "'pth_string':'FEB_20_model_lambda_D_MLE_SILU.pth'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a166ce5-25b3-4d04-9fb1-e2de76ca2d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling load_untrained_model({'n_layers': 12, 'hidden_size': 12, 'dropout': 0.13, 'NFEATURES': 3, 'activation': 'SiLU', 'optimizer_name': 'NAdam', 'starting_learning_rate': 0.0006, 'momentum': 0.9, 'batch_size': 512, 'n_iterations': 60000, 'traces_step': 100, 'L2': 0.1, 'MLE': True, 'with_lambda_D': True, 'pth_string': 'FEB_20_model_lambda_D_MLE_SILU.pth'})\n",
      "INITIATED UNTRAINED MODEL:\n",
      "'load_untrained_model' returned SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0.13, inplace=False)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (4): Dropout(p=0.13, inplace=False)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (7): Dropout(p=0.13, inplace=False)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (10): Dropout(p=0.13, inplace=False)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (13): Dropout(p=0.13, inplace=False)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (16): Dropout(p=0.13, inplace=False)\n",
      "    (17): SiLU()\n",
      "    (18): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (19): Dropout(p=0.13, inplace=False)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (22): Dropout(p=0.13, inplace=False)\n",
      "    (23): SiLU()\n",
      "    (24): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (25): Dropout(p=0.13, inplace=False)\n",
      "    (26): SiLU()\n",
      "    (27): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (28): Dropout(p=0.13, inplace=False)\n",
      "    (29): SiLU()\n",
      "    (30): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (31): Dropout(p=0.13, inplace=False)\n",
      "    (32): SiLU()\n",
      "    (33): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (34): Dropout(p=0.13, inplace=False)\n",
      "    (35): SiLU()\n",
      "    (36): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (37): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "untrained_SiLU_model_MLE = load_untrained_model(PARAMS_lambdaD_MLE_SILU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b26c4fb-b0b0-4cb1-9c0c-4fc9e91443f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIATED MODEL: SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0.13, inplace=False)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (4): Dropout(p=0.13, inplace=False)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (7): Dropout(p=0.13, inplace=False)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (10): Dropout(p=0.13, inplace=False)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (13): Dropout(p=0.13, inplace=False)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (16): Dropout(p=0.13, inplace=False)\n",
      "    (17): SiLU()\n",
      "    (18): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (19): Dropout(p=0.13, inplace=False)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (22): Dropout(p=0.13, inplace=False)\n",
      "    (23): SiLU()\n",
      "    (24): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (25): Dropout(p=0.13, inplace=False)\n",
      "    (26): SiLU()\n",
      "    (27): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (28): Dropout(p=0.13, inplace=False)\n",
      "    (29): SiLU()\n",
      "    (30): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (31): Dropout(p=0.13, inplace=False)\n",
      "    (32): SiLU()\n",
      "    (33): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (34): Dropout(p=0.13, inplace=False)\n",
      "    (35): SiLU()\n",
      "    (36): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (37): Sigmoid()\n",
      "  )\n",
      ")\n",
      "loading model with th string : FEB_20_model_lambda_D_MLE_SILU.pth\n",
      "\n",
      "SiLURegressionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=12, bias=True)\n",
      "    (1): Dropout(p=0.13, inplace=False)\n",
      "    (2): SiLU()\n",
      "    (3): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (4): Dropout(p=0.13, inplace=False)\n",
      "    (5): SiLU()\n",
      "    (6): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (7): Dropout(p=0.13, inplace=False)\n",
      "    (8): SiLU()\n",
      "    (9): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (10): Dropout(p=0.13, inplace=False)\n",
      "    (11): SiLU()\n",
      "    (12): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (13): Dropout(p=0.13, inplace=False)\n",
      "    (14): SiLU()\n",
      "    (15): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (16): Dropout(p=0.13, inplace=False)\n",
      "    (17): SiLU()\n",
      "    (18): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (19): Dropout(p=0.13, inplace=False)\n",
      "    (20): SiLU()\n",
      "    (21): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (22): Dropout(p=0.13, inplace=False)\n",
      "    (23): SiLU()\n",
      "    (24): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (25): Dropout(p=0.13, inplace=False)\n",
      "    (26): SiLU()\n",
      "    (27): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (28): Dropout(p=0.13, inplace=False)\n",
      "    (29): SiLU()\n",
      "    (30): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (31): Dropout(p=0.13, inplace=False)\n",
      "    (32): SiLU()\n",
      "    (33): Linear(in_features=12, out_features=12, bias=True)\n",
      "    (34): Dropout(p=0.13, inplace=False)\n",
      "    (35): SiLU()\n",
      "    (36): Linear(in_features=12, out_features=1, bias=True)\n",
      "    (37): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# untrained_SiLU_model = load_untrained_model(PARAMS_lambdaD_nonMLE_SILU)\n",
    "\n",
    "trained_SiLU_model_MLE = load_model(model = untrained_SiLU_model_MLE, \n",
    "                                PARAMS=PARAMS_lambdaD_MLE_SILU,\n",
    "                   pth_string=PARAMS_lambdaD_MLE_SILU[\"pth_string\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e482ade-2016-4087-8c87-c16490292f49",
   "metadata": {},
   "source": [
    "# Summary: We have loaded `trained_SiLU_model_nonMLE` and `trained_SiLU_model_MLE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf514394-e4bd-4eac-87ca-217e3e74cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainedModel:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def __cdf(self, x):\n",
    "        # convert to a tensor and compute\n",
    " \n",
    "        X = torch.Tensor(x)#.transpose(1, 0)\n",
    "            \n",
    "        X.requires_grad_(True)\n",
    "        \n",
    "        self.model.eval() \n",
    "        \n",
    "        return self.model(X), X \n",
    "        \n",
    "    def cdf(self, x): \n",
    "        F, _ = self.__cdf(x)\n",
    "        \n",
    "        Y = F.view(-1).detach().numpy()\n",
    "        if len(Y) == 1:\n",
    "            Y = Y[0]\n",
    "        return Y\n",
    "\n",
    "        \n",
    "        Y = F.view(-1).detach().numpy()\n",
    "        if len(Y) == 1:\n",
    "            Y = Y[0]\n",
    "        return Y\n",
    "    \n",
    "    def numerical_deriv(self,x):\n",
    "        h=1E-3\n",
    "        F_h = self.cdf(x+h)\n",
    "        F = self.cdf(x)\n",
    "        deriv = (F_h-F)/h\n",
    "        return deriv\n",
    "        \n",
    "\n",
    "    def pdf(self, x):\n",
    "        F, X = self.__cdf(x)\n",
    "\n",
    "        dFdX = torch.autograd.grad(outputs=F, inputs=X, \n",
    "                               grad_outputs=torch.ones_like(F),\n",
    "                               #allow_unused=True, \n",
    "                               #retain_graph=True, \n",
    "                               create_graph=True)[0]\n",
    "    \n",
    "        # Y = dFdX.view(-1).detach().numpy()\n",
    "        Y = dFdX.detach().numpy()\n",
    "        if len(Y) == 1:\n",
    "            Y = Y[0]\n",
    "        return Y.T[-1]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3398f7-a379-48c6-be6a-418e0080504c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_X(theta, nu, N_points, MLE):\n",
    "    N = st.poisson.rvs(theta+nu, size=N_points)\n",
    "    M = st.poisson.rvs(nu, size=N_points)\n",
    "    lambda_D = lambda_test_2d(N, M, theta, nu, MLE)#.flatten()\n",
    "    X = np.empty((N_points, 3))\n",
    "    X[:,0] = np.ones_like(theta)*theta\n",
    "    X[:,1] = np.ones_like(nu)*nu\n",
    "    X[:,2]  = lambda_D\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63370db1-bb17-409e-8185-a1ea02a786b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.315048</td>\n",
       "      <td>16.110610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.529150</td>\n",
       "      <td>16.990196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.053639</td>\n",
       "      <td>4.442625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.230171</td>\n",
       "      <td>7.583039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.612315</td>\n",
       "      <td>5.307532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       theta         nu\n",
       "0   3.315048  16.110610\n",
       "1  19.529150  16.990196\n",
       "2  19.053639   4.442625\n",
       "3   3.230171   7.583039\n",
       "4  15.612315   5.307532"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bprime=10\n",
    "thetamin, thetamax  =0,20\n",
    "numin, numax  =0,20\n",
    "theta = st.uniform.rvs(thetamin, thetamax, size=Bprime)\n",
    "nu = st.uniform.rvs(numin, numax, size=Bprime)\n",
    "data = pd.DataFrame({'theta':theta, 'nu':nu})\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f13249e-e265-4456-b4d5-4b11bbd5e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rowind, row in data.iterrows():\n",
    "    X = generate_X(theta=row['theta'], nu=row['nu'], N_points=1000, MLE=False)\n",
    "    lambda_ = X[:-1].flatten()\n",
    "    lambda_ = [float(int(1000*x))/1000 for x in lambda_]\n",
    "    # data.loc[rowind, 'lambda'] = [str(x) for x in lambda_]\n",
    "    data.loc[rowind, 'lambda'] =str(lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c7e394ca-f271-46f7-81a3-bd1ed2c94373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theta</th>\n",
       "      <th>nu</th>\n",
       "      <th>lambda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.315048</td>\n",
       "      <td>16.110610</td>\n",
       "      <td>[3.315, 16.11, 4.234, 3.315, 16.11, 1.392, 3.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.529150</td>\n",
       "      <td>16.990196</td>\n",
       "      <td>[19.529, 16.99, 2.58, 19.529, 16.99, 1.812, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.053639</td>\n",
       "      <td>4.442625</td>\n",
       "      <td>[19.053, 4.442, 5.648, 19.053, 4.442, 7.899, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.230171</td>\n",
       "      <td>7.583039</td>\n",
       "      <td>[3.23, 7.583, 1.282, 3.23, 7.583, 5.068, 3.23,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.612315</td>\n",
       "      <td>5.307532</td>\n",
       "      <td>[15.612, 5.307, 6.009, 15.612, 5.307, 0.544, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16.663557</td>\n",
       "      <td>13.940325</td>\n",
       "      <td>[16.663, 13.94, 3.478, 16.663, 13.94, 0.855, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.525766</td>\n",
       "      <td>9.690538</td>\n",
       "      <td>[15.525, 9.69, 3.511, 15.525, 9.69, 0.887, 15....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19.088251</td>\n",
       "      <td>1.021824</td>\n",
       "      <td>[19.088, 1.021, 0.397, 19.088, 1.021, 0.959, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.443142</td>\n",
       "      <td>15.161666</td>\n",
       "      <td>[7.443, 15.161, 0.617, 7.443, 15.161, 3.052, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.541284</td>\n",
       "      <td>8.406816</td>\n",
       "      <td>[2.541, 8.406, 2.852, 2.541, 8.406, 0.766, 2.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       theta         nu                                             lambda\n",
       "0   3.315048  16.110610  [3.315, 16.11, 4.234, 3.315, 16.11, 1.392, 3.3...\n",
       "1  19.529150  16.990196  [19.529, 16.99, 2.58, 19.529, 16.99, 1.812, 19...\n",
       "2  19.053639   4.442625  [19.053, 4.442, 5.648, 19.053, 4.442, 7.899, 1...\n",
       "3   3.230171   7.583039  [3.23, 7.583, 1.282, 3.23, 7.583, 5.068, 3.23,...\n",
       "4  15.612315   5.307532  [15.612, 5.307, 6.009, 15.612, 5.307, 0.544, 1...\n",
       "5  16.663557  13.940325  [16.663, 13.94, 3.478, 16.663, 13.94, 0.855, 1...\n",
       "6  15.525766   9.690538  [15.525, 9.69, 3.511, 15.525, 9.69, 0.887, 15....\n",
       "7  19.088251   1.021824  [19.088, 1.021, 0.397, 19.088, 1.021, 0.959, 1...\n",
       "8   7.443142  15.161666  [7.443, 15.161, 0.617, 7.443, 15.161, 3.052, 7...\n",
       "9   2.541284   8.406816  [2.541, 8.406, 2.852, 2.541, 8.406, 0.766, 2.5..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b0b0c5d-8650-46b4-a3a2-e9b1c9de725b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.31,\n",
       " 16.11,\n",
       " 0.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.46,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.24,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.87,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.18,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.31,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.69,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.17,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.37,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.75,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.27,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.77,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.06,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.99,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.96,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.56,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.75,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.4,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.12,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.47,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.59,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.9,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.33,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.85,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.35,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.69,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.28,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.46,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.63,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.16,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.59,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.99,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.17,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 7.72,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.64,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.65,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.7,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.34,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.29,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.56,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.62,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.06,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.64,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.69,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.72,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.92,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.88,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.21,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 6.14,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.76,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.33,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.2,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 8.68,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.1,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.71,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.77,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.33,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.38,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.48,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.03,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.06,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.87,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.19,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.75,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.47,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 10.85,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.24,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.86,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.22,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.09,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.29,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.62,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.96,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.32,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.03,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.14,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.88,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 6.02,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.38,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.06,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.46,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.14,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.34,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.47,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.47,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.17,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.32,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.47,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.47,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.94,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.0,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.77,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.77,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.27,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 8.09,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.38,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.36,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.16,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.43,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.21,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.2,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.62,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.8,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 6.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.29,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.8,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.36,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.06,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.1,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.84,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.17,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.49,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.12,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.55,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.37,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.78,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.22,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.62,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.3,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.73,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.07,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.09,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.22,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.87,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.14,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.21,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 8.62,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.28,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.22,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.15,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.21,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.4,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.23,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.83,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.36,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.31,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.67,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.75,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.97,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.2,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.28,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.64,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 7.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 7.42,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.32,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.7,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.84,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.29,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.72,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.81,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.95,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.71,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.62,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.78,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.09,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.71,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.25,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.95,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.91,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.67,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.79,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.8,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.16,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.09,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.03,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.18,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.89,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.72,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.97,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.29,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.78,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.06,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.95,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.77,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.72,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.0,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.63,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.67,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.88,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.26,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.1,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.2,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.64,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.67,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.41,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.73,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.31,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.1,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.63,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 6.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.12,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.85,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.18,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.16,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 7.08,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 7.28,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.16,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 8.34,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.31,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.44,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.12,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.84,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.6,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.61,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.22,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.09,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.36,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.67,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.25,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.44,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.39,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.16,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 6.14,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.08,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.37,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.49,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.26,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 4.44,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.83,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.15,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.32,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.27,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.96,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.2,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.75,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.23,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.29,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.95,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.25,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.05,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.66,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.68,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.25,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.44,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.72,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.3,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.25,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 5.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.26,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.01,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.69,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.43,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.08,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.11,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.22,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.4,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.44,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 12.2,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.46,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 3.03,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.91,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.54,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.87,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.81,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.91,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.93,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.94,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.5,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 2.88,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 1.31,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 6.56,\n",
       " 3.31,\n",
       " 16.11,\n",
       " 0.66,\n",
       " 3.31,\n",
       " ...]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(data['lambda'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da6362-e7da-49e6-9da4-e7a94c9f3776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
